@article{de2000,
  title={Cultural units in cross-cultural research},
  author={De Munck, Victor and Korotayev, Andrey},
  journal={Ethnology},
  pages={335--348},
  year={2000},
  publisher={JSTOR}
}

@article{garfield2019,
  title={Evolutionary models of leadership: Tests and synthesis},
  author={Garfield, Zachary H and Hubbard, Robert L and Hagen, Edward H},
  journal={Human Nature},
  volume={30},
  number={1},
  pages={23--58},
  year={2019},
  publisher={Springer}
}

@inproceedings{marvin2023,
  title={Prompt engineering in large language models},
  author={Marvin, Ggaliwango and Hellen, Nakayiza and Jjingo, Daudi and Nakatumba-Nabende, Joyce},
  booktitle={International conference on data intelligence and cognitive informatics},
  pages={387--402},
  year={2023},
  organization={Springer}
}

@article{knoechel2024,
  title={Principles for responsible AI usage in research},
  author={Knoechel, Tim-Dorian and Schweizer, Konrad and Acar, Oguz A and Akil, Atakan M and Al-Hoorie, Ali H and Buehler, Florian and Elsherif, Mahmoud and Giannini, Alice and Heyselaar, Evelien and Hosseini, Mohammad and others},
  journal={Open Science Framework},
  year={2024}
}

@book{vallely2002,
  title={Guardians of the transcendent: An ethnography of a Jain ascetic community},
  author={Vallely, Anne},
  volume={22},
  year={2002},
  publisher={University of Toronto Press}
}

@book{moller2005,
  title={Ramadan in Java: The joy and jihad of ritual fasting},
  author={M{\"o}ller, Andr{\'e}},
  volume={20},
  year={2005},
  publisher={Lund University}
}

@article{goffi2022,
  title={Respecting cultural diversity in ethics applied to AI: A new approach for a multicultural governance},
  author={Goffi, Emmanuel and Momcilovic, Aco},
  journal={Misi{\'o}n Jur{\'\i}dica},
  volume={15},
  number={23},
  pages={111--122},
  year={2022}
}

@article{hrnvcivr2020,
  title={Identifying post-marital residence patterns in prehistory: A phylogenetic comparative analysis of dwelling size},
  author={Hrn{\v{c}}{\'\i}{\v{r}}, V{\'a}clav and Duda, Pavel and {\v{S}}affa, Gabriel and Kv{\v{e}}tina, Petr and Zrzav{\`y}, Jan},
  journal={PLoS One},
  volume={15},
  number={2},
  pages={e0229363},
  year={2020},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{minocher2019,
  title={Explaining marriage patterns in a globally representative sample through socio-ecology and population history: A Bayesian phylogenetic analysis using a new supertree},
  author={Minocher, Riana and Duda, Pavel and Jaeggi, Adrian V},
  journal={Evolution and human behavior},
  volume={40},
  number={2},
  pages={176--187},
  year={2019},
  publisher={Elsevier}
}

@article{ringen2019,
  title={The evolution of daily food sharing: A Bayesian phylogenetic analysis},
  author={Ringen, Erik J and Duda, Pavel and Jaeggi, Adrian V},
  journal={Evolution and Human Behavior},
  volume={40},
  number={4},
  pages={375--384},
  year={2019},
  publisher={Elsevier}
}

@article{martinez2025,
  title={Using large language models to estimate features of multi-word expressions: Concreteness, valence, arousal},
  author={Mart{\'\i}nez, Gonzalo and Molero, Juan Diego and Gonz{\'a}lez, Sandra and Conde, Javier and Brysbaert, Marc and Reviriego, Pedro},
  journal={Behavior Research Methods},
  volume={57},
  number={1},
  pages={1--11},
  year={2025},
  publisher={Springer}
}

@inproceedings{jajoo2025,
  title={CultureShift: Mapping Temporal Cultural Evolution in Vision-Language Models},
  author={Jajoo, Gautam and Deshpande, Harsh and Chitale, Pranjal A and others},
  booktitle={CVPR 2025 Workshop Vision Language Models For All}
}

@inproceedings{mohta2023,
  title={Are large language models good annotators?},
  author={Mohta, Jay and Ak, Kenan and Xu, Yan and Shen, Mingwei},
  booktitle={Proceedings on},
  pages={38--48},
  year={2023},
  organization={PMLR}
}

@article{rad2023,
  title={From self-deprivation to cooperation: How Ramadan fasting influences risk-aversion and decisions in resource dilemmas},
  author={Rad, Mostafa Salari},
  journal={Current Research in Ecological and Social Psychology},
  volume={5},
  pages={100152},
  year={2023},
  publisher={Elsevier}
}

@article{haibe2020,
  title={Transparency and reproducibility in artificial intelligence},
  author={Haibe-Kains, Benjamin and Adam, George Alexandru and Hosny, Ahmed and Khodakarami, Farnoosh and Massive Analysis Quality Control (MAQC) Society Board of Directors Shraddha Thakkar 35 Kusko Rebecca 36 Sansone Susanna-Assunta 37 Tong Weida 35 Wolfinger Russ D. 38 Mason Christopher E. 39 Jones Wendell 40 Dopazo Joaquin 41 Furlanello Cesare 42 and Waldron, Levi and Wang, Bo and McIntosh, Chris and Goldenberg, Anna and Kundaje, Anshul and others},
  journal={Nature},
  volume={586},
  number={7829},
  pages={E14--E16},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@misc{unesco2023,
  author       = {{UNESCO}},
  title        = {Recommendation on the Ethics of Artificial Intelligence},
  year         = {2023},
  publisher    = {United Nations Educational, Scientific and Cultural Organization},
  url          = {https://unesdoc.unesco.org/ark:/48223/pf0000380455},
  note         = {Accessed: 2025-06-06}
}


@article{urassa2021,
  title={Cross-cultural research must prioritize equitable collaboration},
  author={Urassa, Mark and Lawson, David W and Wamoyi, Joyce and Gurmu, Eshetu and Gibson, Mhairi A and Madhivanan, Purnima and Placek, Caitlyn},
  journal={Nature Human Behaviour},
  volume={5},
  number={6},
  pages={668--671},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@book{lenette2022,
  title={Participatory action research: Ethics and decolonization},
  author={Lenette, Caroline},
  year={2022},
  publisher={Oxford University Press}
}

@article{mboa2023,
  title={We need a decolonized appropriation of AI in Africa},
  author={Mboa Nkoudou, Thomas Herv{\'e}},
  journal={Nature Human Behaviour},
  volume={7},
  number={11},
  pages={1810--1811},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{grossmann2023,
  title={AI and the transformation of social science research},
  author={Grossmann, Igor and Feinberg, Matthew and Parker, Dawn C and Christakis, Nicholas A and Tetlock, Philip E and Cunningham, William A},
  journal={Science},
  volume={380},
  number={6650},
  pages={1108--1109},
  year={2023},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{xiao2023,
  title={Supporting qualitative analysis with large language models: Combining codebook with GPT-3 for deductive coding},
  author={Xiao, Ziang and Yuan, Xingdi and Liao, Q Vera and Abdelghani, Rania and Oudeyer, Pierre-Yves},
  booktitle={Companion proceedings of the 28th international conference on intelligent user interfaces},
  pages={75--78},
  year={2023}
}

@article{balt2024,
  title={Can a zero-shot learning Large Language Model code complex interview data?},
  author={Balt, E and Salmi, S and Bhulai, S and Eikelenboom, M and Gilissen, R and Creemers, D and Popma, A and M{\'e}relle, S},
  journal={European Journal of Public Health},
  volume={34},
  number={Supplement\_3},
  pages={ckae144--1121},
  year={2024},
  publisher={Oxford University Press}
}

@misc{apicella2020,
  title={Beyond WEIRD: A review of the last decade and a look ahead to the global laboratory of the future},
  author={Apicella, Coren and Norenzayan, Ara and Henrich, Joseph},
  journal={Evolution and Human Behavior},
  volume={41},
  number={5},
  pages={319--329},
  year={2020},
  publisher={Elsevier}
}

@article{henrich2010,
  title={Most people are not WEIRD},
  author={Henrich, Joseph and Heine, Steven J and Norenzayan, Ara},
  journal={Nature},
  volume={466},
  number={7302},
  pages={29--29},
  year={2010},
  publisher={Nature Publishing Group UK London}
}

@incollection{fischer2018,
  title={Big data and research opportunities using HRAF databases},
  author={Fischer, Michael D and Ember, Carol R},
  booktitle={Big data in computational social science and humanities},
  pages={323--336},
  year={2018},
  publisher={Springer}
}

@article{singh2020,
  title={Why do religious leaders observe costly prohibitions? Examining taboos on Mentawai shamans},
  author={Singh, Manvir and Henrich, Joseph},
  journal={Evolutionary Human Sciences},
  volume={2},
  pages={e32},
  year={2020},
  publisher={Cambridge University Press}
}

@article{irons2001,
  title={Religion as a hard-to-fake sign of commitment},
  author={Irons, William},
  journal={Evolution and the capacity for commitment},
  volume={292309},
  year={2001}
}

@article{henrich2009,
  title={The evolution of costly displays, cooperation and religion: Credibility enhancing displays and their implications for cultural evolution},
  author={Henrich, Joseph},
  journal={Evolution and human behavior},
  volume={30},
  number={4},
  pages={244--260},
  year={2009},
  publisher={Elsevier}
}

@article{dinicola1990,
  title={Anorexia multiforme: self-starvation in historical and cultural context: Part II: anorexia nervosa as a culture-reactive syndrome1},
  author={Dinicola, Vincenzo F},
  journal={Transcultural Psychiatric Research Review},
  volume={27},
  number={4},
  pages={245--286},
  year={1990},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{longo2021,
  title={Intermittent and periodic fasting, longevity and disease},
  author={Longo, Valter D and Di Tano, Maira and Mattson, Mark P and Guidi, Novella},
  journal={Nature aging},
  volume={1},
  number={1},
  pages={47--59},
  year={2021},
  publisher={Nature Publishing Group US New York}
}

@article{placek2021,
  title={Pregnancy fasting in ramadan: toward a biocultural framework},
  author={Placek, Caitlyn D and Jaykrishna, Poornima and Srinivas, Vijaya and Madhivanan, Purnima},
  journal={Ecology of Food and Nutrition},
  volume={60},
  number={6},
  pages={785--809},
  year={2021},
  publisher={Taylor \& Francis}
}

@article{sosis2007,
  title={Scars for war: Evaluating alternative signaling explanations for cross-cultural variance in ritual costs},
  author={Sosis, Richard and Kress, Howard C and Boster, James S},
  journal={Evolution and human behavior},
  volume={28},
  number={4},
  pages={234--247},
  year={2007},
  publisher={Elsevier}
}

@article{syme2016,
  title={Testing the bargaining vs. inclusive fitness models of suicidal behavior against the ethnographic record},
  author={Syme, Kristen L and Garfield, Zachary H and Hagen, Edward H},
  journal={Evolution and Human Behavior},
  volume={37},
  number={3},
  pages={179--192},
  year={2016},
  publisher={Elsevier}
}

@article{ember2018our,
  title={Our better nature: Does resource stress predict beyond-household sharing?},
  author={Ember, Carol R and Skoggard, Ian and Ringen, Erik J and Farrer, Megan},
  journal={Evolution and Human Behavior},
  volume={39},
  number={4},
  pages={380--391},
  year={2018},
  publisher={Elsevier}
}

@article{garfield2020,
  title={Universal and variable leadership dimensions across human societies},
  author={Garfield, Zachary H and Syme, Kristen L and Hagen, Edward H},
  journal={Evolution and Human Behavior},
  volume={41},
  number={5},
  pages={397--414},
  year={2020},
  publisher={Elsevier}
}

@article{ember1998,
  title={Cross-cultural research},
  author={Ember, Carol R and Ember, Melvin and Peregrine, N},
  journal={Handbook of methods in cultural anthropology},
  pages={647--687},
  year={1998}
}

@book{ember2009,
  title={Cross-cultural research methods},
  author={Ember, Carol R},
  year={2009},
  publisher={Rowman Altamira}
}

@article{ember1991,
  title={Problems of measurement in cross-cultural research using secondary data},
  author={Ember, Carol R and Ross, Marc Howard and Burton, Michael L and Bradley, Candice},
  journal={Behavior Science Research},
  volume={25},
  number={1-4},
  pages={187--216},
  year={1991},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@book{cavalli1981,
  title={Cultural transmission and evolution: A quantitative approach},
  author={Cavalli-Sforza, Luigi Luca and Feldman, Marcus W},
  number={16},
  year={1981},
  publisher={Princeton University Press}
}

@article{wu2022,
  title={A survey of human-in-the-loop for machine learning},
  author={Wu, Xingjiao and Xiao, Luwei and Sun, Yixuan and Zhang, Junhang and Ma, Tianlong and He, Liang},
  journal={Future Generation Computer Systems},
  volume={135},
  pages={364--381},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{masoud2025,
  title={Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede’s Cultural Dimensions},
  author={Masoud, Reem and Liu, Ziquan and Ferianc, Martin and Treleaven, Philip C and Rodrigues, Miguel Rodrigues},
  booktitle={Proceedings of the 31st International Conference on Computational Linguistics},
  pages={8474--8503},
  year={2025}
}

@article{myung2024,
  title={Blend: A benchmark for llms on everyday knowledge in diverse cultures and languages},
  author={Myung, Junho and Lee, Nayeon and Zhou, Yi and Jin, Jiho and Putri, Rifki and Antypas, Dimosthenis and Borkakoty, Hsuvas and Kim, Eunsu and Perez-Almendros, Carla and Ayele, Abinew Ali and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={78104--78146},
  year={2024}
}

@article{pawar2025,
  title={Survey of cultural awareness in language models: Text and beyond},
  author={Pawar, Siddhesh and Park, Junyeong and Jin, Jiho and Arora, Arnav and Myung, Junho and Yadav, Srishti and Haznitrama, Faiz Ghifari and Song, Inhwa and Oh, Alice and Augenstein, Isabelle},
  journal={Computational Linguistics},
  pages={1--96},
  year={2025},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{gilardi2023,
  title={ChatGPT outperforms crowd workers for text-annotation tasks},
  author={Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\"e}l},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={30},
  pages={e2305016120},
  year={2023},
  publisher={National Academy of Sciences}
}

@article{balt2025,
  title={Deductively coding psychosocial autopsy interview data using a few-shot learning large language model},
  author={Balt, Elias and Salmi, Salim and Bhulai, Sandjai and Vrinzen, Stefan and Eikelenboom, Merijn and Gilissen, Renske and Creemers, Daan and Popma, Arne and M{\'e}relle, Saskia},
  journal={Frontiers in Public Health},
  volume={13},
  pages={1512537},
  year={2025},
  publisher={Frontiers Media SA}
}

@inproceedings{rietz2020,
  title={Cody: An interactive machine learning system for qualitative coding},
  author={Rietz, Tim and Toreini, Peyman and Maedche, Alexander},
  booktitle={Adjunct Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
  pages={90--92},
  year={2020}
}

article{rathje2024,
  title={GPT is an effective tool for multilingual psychological text analysis},
  author={Rathje, Steve and Mirea, Dan-Mircea and Sucholutsky, Ilia and Marjieh, Raja and Robertson, Claire E and Van Bavel, Jay J},
  journal={Proceedings of the National Academy of Sciences},
  volume={121},
  number={34},
  pages={e2308950121},
  year={2024},
  publisher={National Academy of Sciences}
}

@misc{bail2023,
  title={Can generative AI improve social science research},
  author={Bail, CA},
  year={2023}
}

@article{dubourg2024,
  title={A step-by-step method for cultural annotation by LLMs},
  author={Dubourg, Edgar and Thouzeau, Valentin and Baumard, Nicolas},
  journal={Frontiers in Artificial Intelligence},
  volume={7},
  pages={1365508},
  year={2024},
  publisher={Frontiers Media SA}
}

@inproceedings{hou_prompt-based_2024,
	address = {Kyoto Japan},
	title = {Prompt-based and {Fine}-tuned {GPT} {Models} for {Context}-{Dependent} and -{Independent} {Deductive} {Coding} in {Social} {Annotation}},
	isbn = {9798400716188},
	url = {https://dl.acm.org/doi/10.1145/3636555.3636910},
	doi = {10.1145/3636555.3636910},
	language = {en},
	urldate = {2024-07-22},
	booktitle = {Proceedings of the 14th {Learning} {Analytics} and {Knowledge} {Conference}},
	publisher = {ACM},
	author = {Hou, Chenyu and Zhu, Gaoxia and Zheng, Juan and Zhang, Lishan and Huang, Xiaoshan and Zhong, Tianlong and Li, Shan and Du, Hanxiang and Ker, Chin Lee},
	month = mar,
	year = {2024},
	pages = {518--528},
	file = {Available Version (via Google Scholar):/Users/kristensyme/Zotero/storage/LU8W9B76/Hou et al. - 2024 - Prompt-based and Fine-tuned GPT Models for Context.pdf:application/pdf},
}

@article{noauthor_living_2023,
	title = {Living in a brave new {AI} era},
	volume = {7},
	copyright = {2023 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-023-01775-7},
	doi = {10.1038/s41562-023-01775-7},
	abstract = {Although artificial intelligence (AI) was already ubiquitous, the recent arrival of generative AI has ushered in a new era of possibilities as well as risks. This Focus explores the wide-ranging impacts of AI tools on science and society, examining both their potential and their pitfalls.},
	language = {en},
	number = {11},
	urldate = {2024-07-25},
	journal = {Nature Human Behaviour},
	month = nov,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Behavioral Sciences, Experimental Psychology, general, Life Sciences, Microeconomics, Neurosciences, Personality and Social Psychology},
	pages = {1799--1799},
	file = {Full Text PDF:/Users/kristensyme/Zotero/storage/CCVFACQ4/2023 - Living in a brave new AI era.pdf:application/pdf},
}

@article{suzuki_we_2023,
	title = {We need a culturally aware approach to {AI}},
	volume = {7},
	copyright = {2023 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-023-01738-y},
	doi = {10.1038/s41562-023-01738-y},
	abstract = {In Japan, people express gratitude towards technology and this helps them to achieve balance. Yet, dominant narratives teach us that anthropomorphizing artificial intelligence (AI) is not healthy. Our attitudes towards AI should not be bult upon overarching universal models, argues Shoko Suzuki.},
	language = {en},
	number = {11},
	urldate = {2024-07-25},
	journal = {Nature Human Behaviour},
	author = {Suzuki, Shoko},
	month = nov,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cultural and media studies, Ethics},
	pages = {1816--1817},
	file = {Full Text PDF:/Users/kristensyme/Zotero/storage/B8DGK7L8/Suzuki - 2023 - We need a culturally aware approach to AI.pdf:application/pdf},
}

@article{mboa_nkoudou_we_2023,
	title = {We need a decolonized appropriation of {AI} in {Africa}},
	volume = {7},
	copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-023-01741-3},
	doi = {10.1038/s41562-023-01741-3},
	language = {en},
	number = {11},
	urldate = {2024-07-25},
	journal = {Nature Human Behaviour},
	author = {Mboa Nkoudou and Hervé, Thomas},
	month = nov,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cultural and media studies, Development studies, Science, technology and society},
	pages = {1810--1811},
	file = {Full Text PDF:/Users/kristensyme/Zotero/storage/MWRWWTF9/Mboa Nkoudou and Hervé - 2023 - We need a decolonized appropriation of AI in Afric.pdf:application/pdf},
}

@article{chemero_llms_2023,
	title = {{LLMs} differ from human cognition because they are not embodied},
	volume = {7},
	copyright = {2023 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-023-01723-5},
	doi = {10.1038/s41562-023-01723-5},
	abstract = {Large language models (LLMs) are impressive technological creations but they cannot replace all scientific theories of cognition. A science of cognition must focus on humans as embodied, social animals who are embedded in material, cultural and technological contexts.},
	language = {en},
	number = {11},
	urldate = {2024-07-25},
	journal = {Nature Human Behaviour},
	author = {Chemero, Anthony},
	month = nov,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Philosophy, Science, technology and society},
	pages = {1828--1829},
	file = {Full Text PDF:/Users/kristensyme/Zotero/storage/XHZG7ZEM/Chemero - 2023 - LLMs differ from human cognition because they are .pdf:application/pdf},
}

@article{nakadai_ai_2023,
	title = {{AI} language tools risk scientific diversity and innovation},
	volume = {7},
	copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-023-01652-3},
	doi = {10.1038/s41562-023-01652-3},
	language = {en},
	number = {11},
	urldate = {2024-07-25},
	journal = {Nature Human Behaviour},
	author = {Nakadai, Ryosuke and Nakawake, Yo and Shibasaki, Shota},
	month = nov,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cultural evolution, Scientific community},
	pages = {1804--1805},
	file = {Full Text PDF:/Users/kristensyme/Zotero/storage/DV8GMVKP/Nakadai et al. - 2023 - AI language tools risk scientific diversity and in.pdf:application/pdf},
}

@misc{mcintosh_inadequacies_2024,
	title = {Inadequacies of {Large} {Language} {Model} {Benchmarks} in the {Era} of {Generative} {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2402.09880},
	abstract = {The rapid rise in popularity of Large Language Models (LLMs) with emerging capabilities has spurred public curiosity to evaluate and compare different LLMs, leading many researchers to propose their LLM benchmarks. Noticing preliminary inadequacies in those benchmarks, we embarked on a study to critically assess 23 state-of-the-art LLM benchmarks, using our novel unified evaluation framework through the lenses of people, process, and technology, under the pillars of functionality and security. Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment. Our discussions emphasized the urgent need for standardized methodologies, regulatory certainties, and ethical guidelines in light of Artificial Intelligence (AI) advancements, including advocating for an evolution from static benchmarks to dynamic behavioral profiling to accurately capture LLMs' complex behaviors and potential risks. Our study highlighted the necessity for a paradigm shift in LLM evaluation methodologies, underlining the importance of collaborative efforts for the development of universally accepted benchmarks and the enhancement of AI systems' integration into society.},
	urldate = {2024-07-26},
	publisher = {arXiv},
	author = {McIntosh, Timothy R. and Susnjak, Teo and Liu, Tong and Watters, Paul and Halgamuge, Malka N.},
	month = feb,
	year = {2024},
	note = {arXiv:2402.09880 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:/Users/kristensyme/Zotero/storage/RZ5FW4AB/McIntosh et al. - 2024 - Inadequacies of Large Language Model Benchmarks in.pdf:application/pdf;arXiv.org Snapshot:/Users/kristensyme/Zotero/storage/RNQG6YWB/2402.html:text/html},
}

@inproceedings{chen_reflections_2024,
	title = {Reflections \& {Resonance}: {Two}-{Agent} {Partnership} for {Advancing} {LLM}-based {Story} {Annotation}},
	shorttitle = {Reflections \& {Resonance}},
	url = {https://aclanthology.org/2024.lrec-main.1206/},
	urldate = {2024-07-26},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	author = {Chen, Yuetian and Si, Mei},
	year = {2024},
	pages = {13813--13818},
	file = {Available Version (via Google Scholar):/Users/kristensyme/Zotero/storage/NW8RFABE/Chen and Si - 2024 - Reflections & Resonance Two-Agent Partnership for.pdf:application/pdf},
}

@article{peacey_cultural_2024,
	title = {The cultural evolution of witchcraft beliefs},
	volume = {45},
	issn = {1090-5138},
	url = {https://www.sciencedirect.com/science/article/pii/S1090513824000862},
	doi = {10.1016/j.evolhumbehav.2024.106610},
	abstract = {Witchcraft beliefs are historically and geographically widespread, but little is known about the cultural inheritance processes that may explain their variation between populations. A core component of witchcraft belief is that certain people (‘witches’) are thought to harm others using supernatural means. Various traits, which we refer to as the ‘witchcraft phenotype’ accompany these beliefs. Some can be classified as ‘symbolic culture’, including ideas about the typical behaviour of witches and concepts such as familiars (witches' magical helpers), and demographic traits such as the age and sex of those likely to be accused. We conducted an exploratory study of the cultural evolution of 31 witchcraft traits to examine their inferred ancestry and associations with historic population movements. We coded a dataset from ethnographic accounts of Bantu and Bantoid-speaking societies in sub-Saharan Africa (N = 84) and analysed it using phylogenetic comparative methods (PCMs). Our results estimate that while some traits, such as an ordeal to test for witchcraft, have deep history, others, such as accusations of children, may have evolved more recently, or are limited to specific clusters of societies. Demographic and symbolic cultural traits do not typically co-evolve. Our findings suggest traits have different transmission patterns, and these may result from benefits they provide or from universal psychological mechanisms that produce their recurrent evolution.},
	number = {5},
	urldate = {2024-08-04},
	journal = {Evolution and Human Behavior},
	author = {Peacey, Sarah and Wu, Baihui and Grollemund, Rebecca and Mace, Ruth},
	month = sep,
	year = {2024},
	keywords = {Ancestral states, Cultural evolution, Phylogeny, Witchcraft belief},
	pages = {106610},
	file = {ScienceDirect Snapshot:/Users/kristensyme/Zotero/storage/U58QVT79/S1090513824000862.html:text/html},
}

@article{watson-jones_social_2016,
	title = {The {Social} {Functions} of {Group} {Rituals}},
	volume = {25},
	issn = {0963-7214},
	url = {https://doi.org/10.1177/0963721415618486},
	doi = {10.1177/0963721415618486},
	abstract = {Convergent developments across social scientific disciplines provide evidence that ritual is a psychologically prepared, culturally inherited, behavioral trademark of our species. We draw on evidence from the anthropological and evolutionary-science literatures to offer a psychological account of the social functions of ritual for group behavior. Solving the adaptive problems associated with group living requires psychological mechanisms for identifying group members, ensuring their commitment to the group, facilitating cooperation with coalitions, and maintaining group cohesion. The intersection of these lines of inquiry yields new avenues for theory and research on the evolution and ontogeny of social group cognition.},
	language = {en},
	number = {1},
	urldate = {2024-09-05},
	journal = {Current Directions in Psychological Science},
	author = {Watson-Jones, Rachel E. and Legare, Cristine H.},
	month = feb,
	year = {2016},
	note = {Publisher: SAGE Publications Inc},
	pages = {42--46},
	file = {SAGE PDF Full Text:/Users/kristensyme/Zotero/storage/IVNFGZ22/Watson-Jones and Legare - 2016 - The Social Functions of Group Rituals.pdf:application/pdf},
}

@misc{zheng_large_2024,
	title = {Large {Language} {Models} as {Reliable} {Knowledge} {Bases}?},
	url = {http://arxiv.org/abs/2407.13578},
	abstract = {The NLP community has recently shown a growing interest in leveraging Large Language Models (LLMs) for knowledge-intensive tasks, viewing LLMs as potential knowledge bases (KBs). However, the reliability and extent to which LLMs can function as KBs remain underexplored. While previous studies suggest LLMs can encode knowledge within their parameters, the amount of parametric knowledge alone is not sufficient to evaluate their effectiveness as KBs. This study defines criteria that a reliable LLM-as-KB should meet, focusing on factuality and consistency, and covering both seen and unseen knowledge. We develop several metrics based on these criteria and use them to evaluate 26 popular LLMs, while providing a comprehensive analysis of the effects of model size, instruction tuning, and in-context learning (ICL). Our results paint a worrying picture. Even a high-performant model like GPT-3.5-turbo is not factual or consistent, and strategies like ICL and fine-tuning are unsuccessful at making LLMs better KBs.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Zheng, Danna and Lapata, Mirella and Pan, Jeff Z.},
	month = jul,
	year = {2024},
	note = {arXiv:2407.13578 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/kristensyme/Zotero/storage/7QTK975B/Zheng et al. - 2024 - Large Language Models as Reliable Knowledge Bases.pdf:application/pdf;arXiv.org Snapshot:/Users/kristensyme/Zotero/storage/BT5Z4GTQ/2407.html:text/html},
}

@misc{li_quantifying_2024,
	title = {Quantifying {AI} {Psychology}: {A} {Psychometrics} {Benchmark} for {Large} {Language} {Models}},
	shorttitle = {Quantifying {AI} {Psychology}},
	url = {http://arxiv.org/abs/2406.17675},
	abstract = {Large Language Models (LLMs) have demonstrated exceptional task-solving capabilities, increasingly adopting roles akin to human-like assistants. The broader integration of LLMs into society has sparked interest in whether they manifest psychological attributes, and whether these attributes are stable-inquiries that could deepen the understanding of their behaviors. Inspired by psychometrics, this paper presents a framework for investigating psychology in LLMs, including psychological dimension identification, assessment dataset curation, and assessment with results validation. Following this framework, we introduce a comprehensive psychometrics benchmark for LLMs that covers six psychological dimensions: personality, values, emotion, theory of mind, motivation, and intelligence. This benchmark includes thirteen datasets featuring diverse scenarios and item types. Our findings indicate that LLMs manifest a broad spectrum of psychological attributes. We also uncover discrepancies between LLMs' self-reported traits and their behaviors in real-world scenarios. This paper demonstrates a thorough psychometric assessment of LLMs, providing insights into reliable evaluation and potential applications in AI and social sciences.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Li, Yuan and Huang, Yue and Wang, Hongyi and Zhang, Xiangliang and Zou, James and Sun, Lichao},
	month = jun,
	year = {2024},
	note = {arXiv:2406.17675 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/kristensyme/Zotero/storage/9E26HZE7/Li et al. - 2024 - Quantifying AI Psychology A Psychometrics Benchma.pdf:application/pdf;arXiv.org Snapshot:/Users/kristensyme/Zotero/storage/HQN2U98C/2406.html:text/html},
}

@misc{rytting2024,
	title = {Towards {Coding} {Social} {Science} {Datasets} with {Language} {Models}},
	url = {http://arxiv.org/abs/2306.02177},
	abstract = {Researchers often rely on humans to code (label, annotate, etc.) large sets of texts. This kind of human coding forms an important part of social science research, yet the coding process is both resource intensive and highly variable from application to application. In some cases, efforts to automate this process have achieved human-level accuracies, but to achieve this, these attempts frequently rely on thousands of hand-labeled training examples, which makes them inapplicable to small-scale research studies and costly for large ones. Recent advances in a specific kind of artificial intelligence tool - language models (LMs) - provide a solution to this problem. Work in computer science makes it clear that LMs are able to classify text, without the cost (in financial terms and human effort) of alternative methods. To demonstrate the possibilities of LMs in this area of political science, we use GPT-3, one of the most advanced LMs, as a synthetic coder and compare it to human coders. We find that GPT-3 can match the performance of typical human coders and offers benefits over other machine learning methods of coding text. We find this across a variety of domains using very different coding procedures. This provides exciting evidence that language models can serve as a critical advance in the coding of open-ended texts in a variety of applications.},
	urldate = {2024-09-10},
	publisher = {arXiv},
	author = {Rytting, Christopher Michael and Sorensen, Taylor and Argyle, Lisa and Busby, Ethan and Fulda, Nancy and Gubler, Joshua and Wingate, David},
	month = jun,
	year = {2023},
	note = {arXiv:2306.02177 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/kristensyme/Zotero/storage/2JVMVIBZ/Rytting et al. - 2023 - Towards Coding Social Science Datasets with Langua.pdf:application/pdf;arXiv.org Snapshot:/Users/kristensyme/Zotero/storage/8Q8W6KZJ/2306.html:text/html},
}

@misc{chen_empowering_2023,
	title = {Empowering {Psychotherapy} with {Large} {Language} {Models}: {Cognitive} {Distortion} {Detection} through {Diagnosis} of {Thought} {Prompting}},
	shorttitle = {Empowering {Psychotherapy} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.07146},
	abstract = {Mental illness remains one of the most critical public health issues of our time, due to the severe scarcity and accessibility limit of professionals. Psychotherapy requires high-level expertise to conduct deep, complex reasoning and analysis on the cognition modeling of the patients. In the era of Large Language Models, we believe it is the right time to develop AI assistance for computational psychotherapy. We study the task of cognitive distortion detection and propose the Diagnosis of Thought (DoT) prompting. DoT performs diagnosis on the patient's speech via three stages: subjectivity assessment to separate the facts and the thoughts; contrastive reasoning to elicit the reasoning processes supporting and contradicting the thoughts; and schema analysis to summarize the cognition schemas. The generated diagnosis rationales through the three stages are essential for assisting the professionals. Experiments demonstrate that DoT obtains significant improvements over ChatGPT for cognitive distortion detection, while generating high-quality rationales approved by human experts.},
	urldate = {2024-09-10},
	publisher = {arXiv},
	author = {Chen, Zhiyu and Lu, Yujie and Wang, William Yang},
	month = oct,
	year = {2023},
	note = {arXiv:2310.07146 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/kristensyme/Zotero/storage/D6JHLHDH/Chen et al. - 2023 - Empowering Psychotherapy with Large Language Model.pdf:application/pdf;arXiv.org Snapshot:/Users/kristensyme/Zotero/storage/BVWVPMXR/2310.html:text/html},
}

@article{rathje2024,
	title = {{GPT} is an effective tool for multilingual psychological text analysis},
	volume = {121},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/10.1073/pnas.2308950121},
	doi = {10.1073/pnas.2308950121},
	abstract = {The social and behavioral sciences have been increasingly using automated text analysis to measure psychological constructs in text. We explore whether GPT, the large-language model (LLM) underlying the AI chatbot ChatGPT, can be used as a tool for automated psychological text analysis in several languages. Across 15 datasets (
              n
              = 47,925 manually annotated tweets and news headlines), we tested whether different versions of GPT (3.5 Turbo, 4, and 4 Turbo) can accurately detect psychological constructs (sentiment, discrete emotions, offensiveness, and moral foundations) across 12 languages. We found that GPT (
              r
              = 0.59 to 0.77) performed much better than English-language dictionary analysis (
              r
              = 0.20 to 0.30) at detecting psychological constructs as judged by manual annotators. GPT performed nearly as well as, and sometimes better than, several top-performing fine-tuned machine learning models. Moreover, GPT’s performance improved across successive versions of the model, particularly for lesser-spoken languages, and became less expensive. Overall, GPT may be superior to many existing methods of automated text analysis, since it achieves relatively high accuracy across many languages, requires no training data, and is easy to use with simple prompts (e.g., “is this text negative?”) and little coding experience. We provide sample code and a video tutorial for analyzing text with the GPT application programming interface. We argue that GPT and other LLMs help democratize automated text analysis by making advanced natural language processing capabilities more accessible, and may help facilitate more cross-linguistic research with understudied languages.},
	language = {en},
	number = {34},
	urldate = {2024-09-10},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Rathje, Steve and Mirea, Dan-Mircea and Sucholutsky, Ilia and Marjieh, Raja and Robertson, Claire E. and Van Bavel, Jay J.},
	month = aug,
	year = {2024},
	pages = {e2308950121},
	file = {Available Version (via Google Scholar):/Users/kristensyme/Zotero/storage/XATHKCHU/Rathje et al. - 2024 - GPT is an effective tool for multilingual psycholo.pdf:application/pdf},
}

@misc{wu_concept-guided_2023,
	title = {Concept-{Guided} {Chain}-of-{Thought} {Prompting} for {Pairwise} {Comparison} {Scaling} of {Texts} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.12049},
	abstract = {Existing text scaling methods often require a large corpus, struggle with short texts, or require labeled data. We develop a text scaling method that leverages the pattern recognition capabilities of generative large language models (LLMs). Specifically, we propose concept-guided chain-of-thought (CGCoT), which uses prompts designed to summarize ideas and identify target parties in texts to generate concept-specific breakdowns, in many ways similar to guidance for human coder content analysis. CGCoT effectively shifts pairwise text comparisons from a reasoning problem to a pattern recognition problem. We then pairwise compare concept-specific breakdowns using an LLM. We use the results of these pairwise comparisons to estimate a scale using the Bradley-Terry model. We use this approach to scale affective speech on Twitter. Our measures correlate more strongly with human judgments than alternative approaches like Wordfish. Besides a small set of pilot data to develop the CGCoT prompts, our measures require no additional labeled data and produce binary predictions comparable to a RoBERTa-Large model fine-tuned on thousands of human-labeled tweets. We demonstrate how combining substantive knowledge with LLMs can create state-of-the-art measures of abstract concepts.},
	urldate = {2024-09-10},
	publisher = {arXiv},
	author = {Wu, Patrick Y. and Nagler, Jonathan and Tucker, Joshua A. and Messing, Solomon},
	month = oct,
	year = {2023},
	note = {arXiv:2310.12049 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:/Users/kristensyme/Zotero/storage/A2LIK82M/Wu et al. - 2023 - Concept-Guided Chain-of-Thought Prompting for Pair.pdf:application/pdf;arXiv.org Snapshot:/Users/kristensyme/Zotero/storage/5LDXCNR9/2310.html:text/html},
}

@article{hur_language_2024,
	title = {Language sentiment predicts changes in depressive symptoms},
	volume = {121},
	url = {https://www.pnas.org/doi/10.1073/pnas.2321321121},
	doi = {10.1073/pnas.2321321121},
	abstract = {The prevalence of depression is a major societal health concern, and there is an ongoing need to develop tools that predict who will become depressed. Past research suggests that depression changes the language we use, but it is unclear whether language is predictive of worsening symptoms. Here, we test whether the sentiment of brief written linguistic responses predicts changes in depression. Across two studies (N = 467), participants provided responses to neutral open-ended questions, narrating aspects of their lives relevant to depression (e.g., mood, motivation, sleep). Participants also completed the Patient Health Questionnaire (PHQ-9) to assess depressive symptoms and a risky decision-making task with periodic measurements of momentary happiness to quantify mood dynamics. The sentiment of written responses was evaluated by human raters (N = 470), Large Language Models (LLMs; ChatGPT 3.5 and 4.0), and the Linguistic Inquiry and Word Count (LIWC) tool. We found that language sentiment evaluated by human raters and LLMs, but not LIWC, predicted changes in depressive symptoms at a three-week follow-up. Using computational modeling, we found that language sentiment was associated with current mood, but language sentiment predicted symptom changes even after controlling for current mood. In summary, we demonstrate a scalable tool that combines brief written responses with sentiment analysis by AI tools that matches human performance in the prediction of future psychiatric symptoms.},
	number = {39},
	urldate = {2024-09-18},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Hur, Jihyun K. and Heffner, Joseph and Feng, Gloria W. and Joormann, Jutta and Rutledge, Robb B.},
	month = sep,
	year = {2024},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2321321121},
	file = {Full Text PDF:/Users/kristensyme/Zotero/storage/2XCB2RIZ/Hur et al. - 2024 - Language sentiment predicts changes in depressive .pdf:application/pdf},
}

@misc{edwards_language_2024,
	title = {Language {Models} for {Text} {Classification}: {Is} {In}-{Context} {Learning} {Enough}?},
	shorttitle = {Language {Models} for {Text} {Classification}},
	url = {http://arxiv.org/abs/2403.17661},
	doi = {10.48550/arXiv.2403.17661},
	abstract = {Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero- and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand instructions written in natural language (prompts), which helps them generalise better to different tasks and domains without the need for specific training data. This makes them suitable for addressing text classification problems for domains with limited amounts of annotated instances. However, existing research is limited in scale and lacks understanding of how text generation models combined with prompting techniques compare to more established methods for text classification such as fine-tuning masked language models. In this paper, we address this research gap by performing a large-scale evaluation study for 16 text classification datasets covering binary, multiclass, and multilabel problems. In particular, we compare zero- and few-shot approaches of large language models to fine-tuning smaller language models. We also analyse the results by prompt, classification type, domain, and number of labels. In general, the results show how fine-tuning smaller and more efficient language models can still outperform few-shot approaches of larger language models, which have room for improvement when it comes to text classification.},
	urldate = {2025-02-25},
	publisher = {arXiv},
	author = {Edwards, Aleksandra and Camacho-Collados, Jose},
	month = apr,
	year = {2024},
	note = {arXiv:2403.17661 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/kristensyme/Zotero/storage/4H5YWQ99/Edwards and Camacho-Collados - 2024 - Language Models for Text Classification Is In-Con.pdf:application/pdf;Snapshot:/Users/kristensyme/Zotero/storage/3EC4YHHB/2403.html:text/html},
}

@misc{milios_-context_2023,
	title = {In-{Context} {Learning} for {Text} {Classification} with {Many} {Labels}},
	url = {http://arxiv.org/abs/2309.10954},
	doi = {10.48550/arXiv.2309.10954},
	abstract = {In-context learning (ICL) using large language models for tasks with many labels is challenging due to the limited context window, which makes it difficult to fit a sufficient number of examples in the prompt. In this paper, we use a pre-trained dense retrieval model to bypass this limitation, giving the model only a partial view of the full label space for each inference call. Testing with recent open-source LLMs (OPT, LLaMA), we set new state of the art performance in few-shot settings for three common intent classification datasets, with no finetuning. We also surpass fine-tuned performance on fine-grained sentiment classification in certain cases. We analyze the performance across number of in-context examples and different model scales, showing that larger models are necessary to effectively and consistently make use of larger context lengths for ICL. By running several ablations, we analyze the model's use of: a) the similarity of the in-context examples to the current input, b) the semantic content of the class names, and c) the correct correspondence between examples and labels. We demonstrate that all three are needed to varying degrees depending on the domain, contrary to certain recent works.},
	urldate = {2025-02-25},
	publisher = {arXiv},
	author = {Milios, Aristides and Reddy, Siva and Bahdanau, Dzmitry},
	month = dec,
	year = {2023},
	note = {arXiv:2309.10954 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/kristensyme/Zotero/storage/4LGGU2SU/Milios et al. - 2023 - In-Context Learning for Text Classification with M.pdf:application/pdf;Snapshot:/Users/kristensyme/Zotero/storage/QB8LWLGJ/2309.html:text/html},
}

@inproceedings{hamerlik_chatgpt_2024,
	title = {{ChatGPT} as {Your} n-th {Annotator}: {Experiments} in {Leveraging} {Large} {Language} {Models} for {Social} {Science} {Text} {Annotation} in {Slovak} {Language}},
	shorttitle = {{ChatGPT} as {Your} n-th {Annotator}},
	url = {https://aclanthology.org/2024.cpss-1.6/},
	urldate = {2025-02-25},
	booktitle = {Proceedings of the 4th {Workshop} on {Computational} {Linguistics} for the {Political} and {Social} {Sciences}: {Long} and short papers},
	author = {Hamerlik, Endre and Šuppa, Marek and Blšták, Miroslav and Kubík, Jozef and Takáč, Martin and Šimko, Marián and Findor, Andrej},
	year = {2024},
	pages = {81--89},
	file = {Available Version (via Google Scholar):/Users/kristensyme/Zotero/storage/Q9LEYW7X/Hamerlik et al. - 2024 - ChatGPT as Your n-th Annotator Experiments in Lev.pdf:application/pdf},
}

@misc{chen_large_2025,
	title = {Large language models streamline automated systematic review: {A} preliminary study},
	shorttitle = {Large language models streamline automated systematic review},
	url = {http://arxiv.org/abs/2502.15702},
	doi = {10.48550/arXiv.2502.15702},
	abstract = {Large Language Models (LLMs) have shown promise in natural language processing tasks, with the potential to automate systematic reviews. This study evaluates the performance of three state-of-the-art LLMs in conducting systematic review tasks. We assessed GPT-4, Claude-3, and Mistral 8x7B across four systematic review tasks: study design formulation, search strategy development, literature screening, and data extraction. Sourced from a previously published systematic review, we provided reference standard including standard PICO (Population, Intervention, Comparison, Outcome) design, standard eligibility criteria, and data from 20 reference literature. Three investigators evaluated the quality of study design and eligibility criteria using 5-point Liker Scale in terms of accuracy, integrity, relevance, consistency and overall performance. For other tasks, the output is defined as accurate if it is the same as the reference standard. Search strategy performance was evaluated through accuracy and retrieval efficacy. Screening accuracy was assessed for both abstracts screening and full texts screening. Data extraction accuracy was evaluated across 1,120 data points comprising 3,360 individual fields. Claude-3 demonstrated superior overall performance in PICO design. In search strategy formulation, GPT-4 and Claude-3 achieved comparable accuracy, outperforming Mistral. For abstract screening, GPT-4 achieved the highest accuracy, followed by Mistral and Claude-3. In data extraction, GPT-4 significantly outperformed other models. LLMs demonstrate potential for automating systematic review tasks, with GPT-4 showing superior performance in search strategy formulation, literature screening and data extraction. These capabilities make them promising assistive tools for researchers and warrant further development and validation in this field.},
	urldate = {2025-03-28},
	publisher = {arXiv},
	author = {Chen, Xi and Zhang, Xue},
	month = jan,
	year = {2025},
	note = {arXiv:2502.15702 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Full Text PDF:/Users/kristensyme/Zotero/storage/54BBXH5A/Chen and Zhang - 2025 - Large language models streamline automated systematic review A preliminary study.pdf:application/pdf;Snapshot:/Users/kristensyme/Zotero/storage/NH83BS3E/2502.html:text/html},
}

@misc{luo_evaluating_2024,
	title = {Evaluating the {Efficacy} of {Large} {Language} {Models} for {Systematic} {Review} and {Meta}-{Analysis} {Screening}},
	url = {http://medrxiv.org/lookup/doi/10.1101/2024.06.03.24308405},
	doi = {10.1101/2024.06.03.24308405},
	abstract = {ABSTRACT
          
            Background
            Systematic reviews and meta-analyses are essential for informed research and policymaking, yet they are typically resource-intensive and time-consuming. Recent advances in artificial intelligence and machine learning offer promising opportunities to streamline these processes.
          
          
            Objective
            To enhance the efficiency of systematic reviews, we explored the automation of various stages using GPT-3.5 Turbo. We assessed the model’s efficacy and performance by comparing it against three expert-conducted reviews across a comprehensive dataset of 24,534 studies.
          
          
            Methods
            The model’s performance was evaluated through a comparison with three expert reviews, utilizing a pseudo-K-folds permutation and a one-tailed ANOVA with an alpha level of 0.05 to ensure statistical validity. Key performance metrics such as accuracy, sensitivity, specificity, predictive values, F1-score, and the Matthews correlation coefficient were analyzed using two sets of prompts.
          
          
            Results
            Our approach significantly streamlined the systematic review process, which typically takes a year, reducing it to a few hours without sacrificing quality. In the initial screening phase, accuracy, specificity, and negative predictive values ranged between 80\% and 95\%. Sensitivity improved markedly during the second screening phase, demonstrating the model’s robustness when provided with more extensive data.
          
          
            Conclusion
            While ongoing refinements are needed, this tool represents a significant advancement in research methodologies, potentially making systematic reviews more accessible to a wider range of researchers.
          
          
            Impact Statement
            Our manuscript presents a novel review screening protocol built using open-source frameworks, which significantly enhances the systematic review process in terms of efficiency and cost-effectiveness. Leveraging the capabilities of GPT and embedding models, our protocol demonstrates the potential to transform a traditionally time-consuming and expensive task into an accelerated and economical operation, all while maintaining high standards of accuracy and reliability.
          
          
            Key Points
            
              
                GPT screening can streamline systematic reviews from a year-long, expensive process to just hours at minimal cost.
              
              
                Validated across different topics, the protocol exhibits high reliability and consistency in study inclusion.
              
              
                The AI-driven process reduces human bias, with prompt optimization considerably improving sensitivity.},
	language = {en},
	urldate = {2025-03-28},
	publisher = {Health Informatics},
	author = {Luo, Ronald and Sastimoglu, Ziya and Faisal, Abu Ilius and Deen, M. Jamal},
	month = jun,
	year = {2024},
	file = {PDF:/Users/kristensyme/Zotero/storage/SGE2K7VU/Luo et al. - 2024 - Evaluating the Efficacy of Large Language Models for Systematic Review and Meta-Analysis Screening.pdf:application/pdf},
}

@article{khraisha_can_2024,
	title = {Can large language models replace humans in the systematic review process? {Evaluating} {GPT}-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages},
	volume = {15},
	issn = {1759-2879, 1759-2887},
	shorttitle = {Can large language models replace humans in the systematic review process?},
	url = {http://arxiv.org/abs/2310.17526},
	doi = {10.1002/jrsm.1715},
	abstract = {Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts - screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prompts, GPT-4's performance was 'almost perfect.' Penalising GPT-4 for missing key studies using highly reliable prompts improved its performance even more. Our findings indicate that, currently, substantial caution should be used if LLMs are being used to conduct systematic reviews, but suggest that, for certain systematic review tasks delivered under reliable prompts, LLMs can rival human performance.},
	number = {4},
	urldate = {2025-03-28},
	journal = {Research Synthesis Methods},
	author = {Khraisha, Qusai and Put, Sophie and Kappenberg, Johanna and Warraitch, Azza and Hadfield, Kristin},
	month = jul,
	year = {2024},
	note = {arXiv:2310.17526 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {616--626},
	file = {Full Text PDF:/Users/kristensyme/Zotero/storage/2UF4FSXU/Khraisha et al. - 2024 - Can large language models replace humans in the systematic review process Evaluating GPT-4's effica.pdf:application/pdf;Snapshot:/Users/kristensyme/Zotero/storage/7FNSWPXL/2310.html:text/html},
}

@misc{noauthor_journal_nodate,
	title = {Journal of {Medical} {Internet} {Research} - {Potential} {Roles} of {Large} {Language} {Models} in the {Production} of {Systematic} {Reviews} and {Meta}-{Analyses}},
	url = {https://www.jmir.org/2024/1/e56780/},
	urldate = {2025-03-28},
	file = {Journal of Medical Internet Research - Potential Roles of Large Language Models in the Production of Systematic Reviews and Meta-Analyses:/Users/kristensyme/Zotero/storage/MCPCS4SB/e56780.html:text/html},
}

@article{lieberum_large_2025,
	title = {Large language models for conducting systematic reviews: on the rise, but not yet ready for use—a scoping review},
	volume = {181},
	issn = {08954356},
	shorttitle = {Large language models for conducting systematic reviews},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435625000794},
	doi = {10.1016/j.jclinepi.2025.111746},
	abstract = {Background and Objectives: Machine learning promises versatile help in the creation of systematic reviews (SRs). Recently, further developments in the form of large language models (LLMs) and their application in SR conduct attracted attention. We aimed at providing an overview of LLM applications in SR conduct in health research.
Methods: We systematically searched MEDLINE, Web of Science, IEEEXplore, ACM Digital Library, Europe PMC (preprints), Google Scholar, and conducted an additional hand search (last search: February 26, 2024). We included scientiﬁc articles in English or German, published from April 2021 onwards, building upon the results of a mapping review that has not yet identiﬁed LLM applications to support SRs. Two reviewers independently screened studies for eligibility; after piloting, 1 reviewer extracted data, checked by another.
Results: Our database search yielded 8054 hits, and we identiﬁed 33 articles from our hand search. We ﬁnally included 37 articles on LLM support. LLM approaches covered 10 of 13 deﬁned SR steps, most frequently literature search (n 5 15, 41\%), study selection (n 5 14, 38\%), and data extraction (n 5 11, 30\%). The mostly recurring LLM was Generative Pretrained Transformer (GPT) (n 5 33, 89\%). Validation studies were predominant (n 5 21, 57\%). In half of the studies, authors evaluated LLM use as promising (n 5 20, 54\%), one-quarter as neutral (n 5 9, 24\%) and one-ﬁfth as nonpromising (n 5 8, 22\%).
Conclusion: Although LLMs show promise in supporting SR creation, fully established or validated applications are often lacking. The rapid increase in research on LLMs for evidence synthesis production highlights their growing relevance. Ó 2025 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).},
	language = {en},
	urldate = {2025-03-28},
	journal = {Journal of Clinical Epidemiology},
	author = {Lieberum, Judith-Lisa and Töws, Markus and Metzendorf, Maria-Inti and Heilmeyer, Felix and Siemens, Waldemar and Haverkamp, Christian and Böhringer, Daniel and Meerpohl, Joerg J. and Eisele-Metzger, Angelika},
	month = may,
	year = {2025},
	pages = {111746},
	file = {PDF:/Users/kristensyme/Zotero/storage/REW8MHLY/Lieberum et al. - 2025 - Large language models for conducting systematic reviews on the rise, but not yet ready for use—a sc.pdf:application/pdf},
}

@misc{mostafapour_evaluating_2024,
	title = {Evaluating {Literature} {Reviews} {Conducted} by {Humans} {Versus} {ChatGPT}: {Comparative} {Study} ({Preprint})},
	shorttitle = {Evaluating {Literature} {Reviews} {Conducted} by {Humans} {Versus} {ChatGPT}},
	url = {http://preprints.jmir.org/preprint/56537},
	doi = {10.2196/preprints.56537},
	abstract = {Background: With the rapid evolution of artificial intelligence (AI), particularly large language models (LLMs) like ChatGPT-4, there is increasing interest in their potential to assist in scholarly tasks, including conducting literature reviews. However, the efficacy of AI-generated reviews compared to traditional human-led approaches remains underexplored.
Objective: This study aims to compare the quality of literature reviews conducted by OpenAI's ChatGPT-4 model with those conducted by human researchers, focusing on the relational dynamics between physicians and patients.
Methods: The study included two literature reviews on the same topic namely, exploring factors affecting relational dynamics between physicians and patients in medico-legal contexts. One review used OpenAI's GPT-4, last updated in September 2021, and the other was conducted by human researchers. The human review involved a comprehensive literature search using medical subject headings and keywords in Ovid MEDLINE followed with a thematic analysis of the literature to synthesize information from selected articles. The AI-generated review employed a new prompt engineering approach, using iterative and sequential prompts to generate results. Comparative analysis was based on qualitative measures such as accuracy, response time, consistency, breadth and depth of knowledge, contextual understanding, and transparency.
Results: GPT-4 produced an extensive list of relational factors rapidly. The AI model demonstrated an impressive breadth of knowledge but exhibited limitations in depth and contextual understanding, occasionally producing irrelevant or incorrect information. In comparison, human researchers provided a more nuanced and contextually relevant review. The comparative analysis assessed the reviews based on criteria including accuracy, response time, consistency, breadth and depth of knowledge, contextual understanding, and transparency. While GPT-4 showed advantages in response time and breadth of knowledge, human-led reviews excelled in accuracy, depth of knowledge, and contextual understanding.
Conclusions: The study suggests that GPT-4, with structured prompt engineering, can be a valuable tool for conducting preliminary literature reviews by providing a broad overview of topics quickly. However, its limitations necessitate careful expert evaluation and refinement, making it an assistant rather than a substitute for human expertise in comprehensive literature reviews. Moreover, this research highlights the potential and limitations of using AI tools like GPT-4 in academic research, particularly in the fields of health services and medical research. It underscores the necessity of combining AI's rapid information retrieval capabilities with human expertise for more accurate and contextually rich scholarly outputs.},
	language = {en},
	urldate = {2025-03-28},
	publisher = {JMIR AI},
	author = {Mostafapour, Mehrnaz and Fortier, Jacqueline H and Pacheco, Karen and Murray, Heather and Garber, Gary},
	month = jan,
	year = {2024},
	file = {PDF:/Users/kristensyme/Zotero/storage/ZDY8RKH3/Mostafapour et al. - 2024 - Evaluating Literature Reviews Conducted by Humans Versus ChatGPT Comparative Study (Preprint).pdf:application/pdf},
}

@article{alshami_harnessing_2023,
	title = {Harnessing the {Power} of {ChatGPT} for {Automating} {Systematic} {Review} {Process}: {Methodology}, {Case} {Study}, {Limitations}, and {Future} {Directions}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-8954},
	shorttitle = {Harnessing the {Power} of {ChatGPT} for {Automating} {Systematic} {Review} {Process}},
	url = {https://www.mdpi.com/2079-8954/11/7/351},
	doi = {10.3390/systems11070351},
	abstract = {Systematic reviews (SR) are crucial in synthesizing and analyzing existing scientific literature to inform evidence-based decision-making. However, traditional SR methods often have limitations, including a lack of automation and decision support, resulting in time-consuming and error-prone reviews. To address these limitations and drive the field forward, we harness the power of the revolutionary language model, ChatGPT, which has demonstrated remarkable capabilities in various scientific writing tasks. By utilizing ChatGPT’s natural language processing abilities, our objective is to automate and streamline the steps involved in traditional SR, explicitly focusing on literature search, screening, data extraction, and content analysis. Therefore, our methodology comprises four modules: (1) Preparation of Boolean research terms and article collection, (2) Abstract screening and articles categorization, (3) Full-text filtering and information extraction, and (4) Content analysis to identify trends, challenges, gaps, and proposed solutions. Throughout each step, our focus has been on providing quantitative analyses to strengthen the robustness of the review process. To illustrate the practical application of our method, we have chosen the topic of IoT applications in water and wastewater management and quality monitoring due to its critical importance and the dearth of comprehensive reviews in this field. The findings demonstrate the potential of ChatGPT in bridging the gap between traditional SR methods and AI language models, resulting in enhanced efficiency and reliability of SR processes. Notably, ChatGPT exhibits exceptional performance in filtering and categorizing relevant articles, leading to significant time and effort savings. Our quantitative assessment reveals the following: (1) the overall accuracy of ChatGPT for article discarding and classification is 88\%, and (2) the F-1 scores of ChatGPT for article discarding and classification are 91\% and 88\%, respectively, compared to expert assessments. However, we identify limitations in its suitability for article extraction. Overall, this research contributes valuable insights to the field of SR, empowering researchers to conduct more comprehensive and reliable reviews while advancing knowledge and decision-making across various domains.},
	language = {en},
	number = {7},
	urldate = {2025-03-28},
	journal = {Systems},
	author = {Alshami, Ahmad and Elsayed, Moustafa and Ali, Eslam and Eltoukhy, Abdelrahman E. E. and Zayed, Tarek},
	month = jul,
	year = {2023},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {article categorization, article filtration, automation, ChatGPT, content analysis, information extraction, Internet of Things (IoT), systematic review},
	pages = {351},
	file = {Full Text PDF:/Users/kristensyme/Zotero/storage/HHI8ZL5X/Alshami et al. - 2023 - Harnessing the Power of ChatGPT for Automating Systematic Review Process Methodology, Case Study, L.pdf:application/pdf},
}

@article{frank_openly_2023,
	title = {Openly accessible {LLMs} can help us to understand human cognition},
	volume = {7},
	copyright = {2023 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-023-01732-4},
	doi = {10.1038/s41562-023-01732-4},
	abstract = {Large language models can be construed as ‘cognitive models’, scientific artefacts that help us to understand the human mind. If made openly accessible, they may provide a valuable model system for studying the emergence of language, reasoning and other uniquely human behaviours.},
	language = {en},
	number = {11},
	urldate = {2025-04-22},
	journal = {Nature Human Behaviour},
	author = {Frank, Michael C.},
	month = nov,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Language and linguistics},
	pages = {1825--1827},
}

@article{chiriatti_case_2024,
	title = {The case for human–{AI} interaction as system 0 thinking},
	volume = {8},
	copyright = {2024 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-024-01995-5},
	doi = {10.1038/s41562-024-01995-5},
	language = {en},
	number = {10},
	urldate = {2025-04-22},
	journal = {Nature Human Behaviour},
	author = {Chiriatti, Massimo and Ganapini, Marianna and Panai, Enrico and Ubiali, Mario and Riva, Giuseppe},
	month = oct,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Information systems and information technology, Information technology},
	pages = {1829--1830},
}

@article{wexler_ethical_2024,
	title = {Ethical challenges in translating brain–computer interfaces},
	volume = {8},
	copyright = {2024 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-024-01972-y},
	doi = {10.1038/s41562-024-01972-y},
	abstract = {Brain–computer interfaces (BCIs) have the potential to revolutionize treatment for individuals with severe disabilities. As these technologies transition from the laboratory to real-world applications, they pose unique ethical challenges that necessitate careful consideration.},
	language = {en},
	number = {10},
	urldate = {2025-04-22},
	journal = {Nature Human Behaviour},
	author = {Wexler, Anna and Feinsinger, Ashley},
	month = oct,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Neural decoding, Ethics, Neuroscience},
	pages = {1831--1833},
}

@article{yan_promises_2024,
	title = {Promises and challenges of generative artificial intelligence for human learning},
	volume = {8},
	copyright = {2024 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-024-02004-5},
	doi = {10.1038/s41562-024-02004-5},
	abstract = {Generative artificial intelligence (GenAI) holds the potential to transform the delivery, cultivation and evaluation of human learning. Here the authors examine the integration of GenAI as a tool for human learning, addressing its promises and challenges from a holistic viewpoint that integrates insights from learning sciences, educational technology and human–computer interaction. GenAI promises to enhance learning experiences by scaling personalized support, diversifying learning materials, enabling timely feedback and innovating assessment methods. However, it also presents critical issues such as model imperfections, ethical dilemmas and the disruption of traditional assessments. Thus, cultivating AI literacy and adaptive skills is imperative for facilitating informed engagement with GenAI technologies. Rigorous research across learning contexts is essential to evaluate GenAI’s effect on human cognition, metacognition and creativity. Humanity must learn with and about GenAI, ensuring that it becomes a powerful ally in the pursuit of knowledge and innovation, rather than a crutch that undermines our intellectual abilities.},
	language = {en},
	number = {10},
	urldate = {2025-04-22},
	journal = {Nature Human Behaviour},
	author = {Yan, Lixiang and Greiff, Samuel and Teuber, Ziwen and Gašević, Dragan},
	month = oct,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Complex networks, Education},
	pages = {1839--1850},
}

@article{collins_building_2024,
	title = {Building machines that learn and think with people},
	volume = {8},
	copyright = {2024 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-024-01991-9},
	doi = {10.1038/s41562-024-01991-9},
	abstract = {What do we want from machine intelligence? We envision machines that are not just tools for thought but partners in thought: reasonable, insightful, knowledgeable, reliable and trustworthy systems that think with us. Current artificial intelligence systems satisfy some of these criteria, some of the time. In this Perspective, we show how the science of collaborative cognition can be put to work to engineer systems that really can be called ‘thought partners’, systems built to meet our expectations and complement our limitations. We lay out several modes of collaborative thought in which humans and artificial intelligence thought partners can engage, and we propose desiderata for human-compatible thought partnerships. Drawing on motifs from computational cognitive science, we motivate an alternative scaling path for the design of thought partners and ecosystems around their use through a Bayesian lens, whereby the partners we construct actively build and reason over models of the human and world.},
	language = {en},
	number = {10},
	urldate = {2025-04-22},
	journal = {Nature Human Behaviour},
	author = {Collins, Katherine M. and Sucholutsky, Ilia and Bhatt, Umang and Chandra, Kartik and Wong, Lionel and Lee, Mina and Zhang, Cedegao E. and Zhi-Xuan, Tan and Ho, Mark and Mansinghka, Vikash and Weller, Adrian and Tenenbaum, Joshua B. and Griffiths, Thomas L.},
	month = oct,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Computer science},
	pages = {1851--1863},
}

@article{steyvers_what_2025,
	title = {What large language models know and what people think they know},
	volume = {7},
	copyright = {2025 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-024-00976-7},
	doi = {10.1038/s42256-024-00976-7},
	abstract = {As artificial intelligence systems, particularly large language models (LLMs), become increasingly integrated into decision-making processes, the ability to trust their outputs is crucial. To earn human trust, LLMs must be well calibrated such that they can accurately assess and communicate the likelihood of their predictions being correct. Whereas recent work has focused on LLMs’ internal confidence, less is understood about how effectively they convey uncertainty to users. Here we explore the calibration gap, which refers to the difference between human confidence in LLM-generated answers and the models’ actual confidence, and the discrimination gap, which reflects how well humans and models can distinguish between correct and incorrect answers. Our experiments with multiple-choice and short-answer questions reveal that users tend to overestimate the accuracy of LLM responses when provided with default explanations. Moreover, longer explanations increased user confidence, even when the extra length did not improve answer accuracy. By adjusting LLM explanations to better reflect the models’ internal confidence, both the calibration gap and the discrimination gap narrowed, significantly improving user perception of LLM accuracy. These findings underscore the importance of accurate uncertainty communication and highlight the effect of explanation length in influencing user trust in artificial-intelligence-assisted decision-making environments.},
	language = {en},
	number = {2},
	urldate = {2025-04-22},
	journal = {Nature Machine Intelligence},
	author = {Steyvers, Mark and Tejeda, Heliodoro and Kumar, Aakriti and Belem, Catarina and Karny, Sheer and Hu, Xinyue and Mayer, Lukas W. and Smyth, Padhraic},
	month = feb,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Computer science},
	pages = {221--231},
	file = {Full Text PDF:/Users/kristensyme/Zotero/storage/FGTFWVCV/Steyvers et al. - 2025 - What large language models know and what people th.pdf:application/pdf},
}

@misc{cai_role_2025,
	title = {The {Role} of {Deductive} and {Inductive} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2410.02892},
	doi = {10.48550/arXiv.2410.02892},
	abstract = {Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning tasks, yet their reliance on static prompt structures and limited adaptability to complex scenarios remains a significant challenge. In this paper, we propose the Deductive and InDuctive(DID) method, a novel framework that enhances LLM reasoning by dynamically integrating both deductive and inductive reasoning approaches. Drawing from cognitive science principles, DID implements a dual-metric complexity evaluation system that combines Littlestone dimension and information entropy to precisely assess task difficulty and guide decomposition strategies. DID enables the model to progressively adapt its reasoning pathways based on problem complexity, mirroring human cognitive processes. We evaluate DID's effectiveness across multiple benchmarks, including the AIW and MR-GSM8K, as well as our custom Holiday Puzzle dataset for temporal reasoning. Our results demonstrate significant improvements in reasoning quality and solution accuracy - achieving 70.3\% accuracy on AIW (compared to 62.2\% for Tree of Thought) while maintaining lower computational costs. The success of DID in improving LLM performance while preserving computational efficiency suggests promising directions for developing more cognitively aligned and capable language models. Our work contributes a theoretically grounded, input-centric approach to enhancing LLM reasoning capabilities, offering an efficient alternative to traditional output-exploration methods.},
	urldate = {2025-04-24},
	publisher = {arXiv},
	author = {Cai, Chengkun and Zhao, Xu and Liu, Haoliang and Jiang, Zhongyu and Zhang, Tianfang and Wu, Zongkai and Hwang, Jenq-Neng and Belongie, Serge and Li, Lei},
	month = feb,
	year = {2025},
	note = {arXiv:2410.02892 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/kristensyme/Zotero/storage/EU94VF5F/Cai et al. - 2025 - The Role of Deductive and Inductive Reasoning in L.pdf:application/pdf;Snapshot:/Users/kristensyme/Zotero/storage/DALMQ26Y/2410.html:text/html},
}

@article{bhatia_inductive_2023,
	title = {Inductive reasoning in minds and machines.},
	url = {https://psycnet.apa.org/record/2024-09766-001},
	urldate = {2025-04-24},
	journal = {Psychological Review},
	author = {Bhatia, Sudeep},
	year = {2023},
	note = {Publisher: American Psychological Association},
	file = {Available Version (via Google Scholar):/Users/kristensyme/Zotero/storage/V4BVDTP3/Bhatia - 2023 - Inductive reasoning in minds and machines..pdf:application/pdf},
}

@article{tai2024,
	title = {An {Examination} of the {Use} of {Large} {Language} {Models} to {Aid} {Analysis} of {Textual} {Data}},
	volume = {23},
	issn = {1609-4069},
	url = {https://doi.org/10.1177/16094069241231168},
	doi = {10.1177/16094069241231168},
	abstract = {The increasing use of machine learning and Large Language Models (LLMs) opens up opportunities to use these artificially intelligent algorithms in novel ways. This article proposes a methodology using LLMs to support traditional deductive coding in qualitative research. We began our analysis with three different sample texts taken from existing interviews. Next, we created a codebook and inputted the sample text and codebook into an LLM. We asked the LLM to determine if the codes were present in a sample text provided and requested evidence to support the coding. The sample texts were inputted 160 times to record changes between iterations of the LLM response. Each iteration was analogous to a new coder deductively analyzing the text with the codebook information. In our results, we present the outputs for these recursive analyses, along with a comparison of the LLM coding to evaluations made by human coders using traditional coding methods. We argue that LLM analysis can aid qualitative researchers by deductively coding transcripts, providing a systematic and reliable platform for code identification, and offering a means of avoiding analysis misalignment. Implications of using LLM in research praxis are discussed, along with current limitations.},
	language = {EN},
	urldate = {2025-04-25},
	journal = {International Journal of Qualitative Methods},
	author = {Tai, Robert H. and Bentley, Lillian R. and Xia, Xin and Sitt, Jason M. and Fankhauser, Sarah C. and Chicas-Mosier, Ana M. and Monteith, Barnas G.},
	month = nov,
	year = {2024},
	note = {Publisher: SAGE Publications Inc},
	pages = {16094069241231168},
	file = {SAGE PDF Full Text:/Users/kristensyme/Zotero/storage/B9PM3MIU/Tai et al. - 2024 - An Examination of the Use of Large Language Models.pdf:application/pdf},
}

@misc{sun_itd_2024,
	title = {{ItD}: {Large} {Language} {Models} {Can} {Teach} {Themselves} {Induction} through {Deduction}},
	shorttitle = {{ItD}},
	url = {http://arxiv.org/abs/2403.05789},
	doi = {10.48550/arXiv.2403.05789},
	abstract = {Although Large Language Models (LLMs) are showing impressive performance on a wide range of Natural Language Processing tasks, researchers have found that they still have limited ability to conduct induction. Recent works mainly adopt ``post processes'' paradigms to improve the performance of LLMs on induction (e.g., the hypothesis search \& refinement methods), but their performance is still constrained by the inherent inductive capability of the LLMs. In this paper, we propose a novel framework, Induction through Deduction (ItD), to enable the LLMs to teach themselves induction through deduction. The ItD framework is composed of two main components: a Deductive Data Generation module to generate induction data and a Naive Bayesian Induction module to optimize the fine-tuning and decoding of LLMs. Our empirical results showcase the effectiveness of ItD on two induction benchmarks, achieving relative performance improvement of 36\% and 10\% compared with previous state-of-the-art, respectively. Our ablation study verifies the effectiveness of two key modules of ItD. We also verify the effectiveness of ItD across different LLMs and deductors. The data and code of this paper can be found at https://anonymous.4open.science/r/ItD-E844.},
	urldate = {2025-04-25},
	publisher = {arXiv},
	author = {Sun, Wangtao and Xu, Haotian and Yu, Xuanqing and Chen, Pei and He, Shizhu and Zhao, Jun and Liu, Kang},
	month = mar,
	year = {2024},
	note = {arXiv:2403.05789 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/kristensyme/Zotero/storage/MXUQ74Q5/Sun et al. - 2024 - ItD Large Language Models Can Teach Themselves In.pdf:application/pdf;Snapshot:/Users/kristensyme/Zotero/storage/CG5GW6P2/2403.html:text/html},
}

@misc{gan_retrieval_2025,
	title = {Retrieval {Augmented} {Generation} {Evaluation} in the {Era} of {Large} {Language} {Models}: {A} {Comprehensive} {Survey}},
	shorttitle = {Retrieval {Augmented} {Generation} {Evaluation} in the {Era} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2504.14891},
	doi = {10.48550/arXiv.2504.14891},
	abstract = {Recent advancements in Retrieval-Augmented Generation (RAG) have revolutionized natural language processing by integrating Large Language Models (LLMs) with external information retrieval, enabling accurate, up-to-date, and verifiable text generation across diverse applications. However, evaluating RAG systems presents unique challenges due to their hybrid architecture that combines retrieval and generation components, as well as their dependence on dynamic knowledge sources in the LLM era. In response, this paper provides a comprehensive survey of RAG evaluation methods and frameworks, systematically reviewing traditional and emerging evaluation approaches, for system performance, factual accuracy, safety, and computational efficiency in the LLM era. We also compile and categorize the RAG-specific datasets and evaluation frameworks, conducting a meta-analysis of evaluation practices in high-impact RAG research. To the best of our knowledge, this work represents the most comprehensive survey for RAG evaluation, bridging traditional and LLM-driven methods, and serves as a critical resource for advancing RAG development.},
	urldate = {2025-04-27},
	publisher = {arXiv},
	author = {Gan, Aoran and Yu, Hao and Zhang, Kai and Liu, Qi and Yan, Wenyu and Huang, Zhenya and Tong, Shiwei and Hu, Guoping},
	month = apr,
	year = {2025},
	note = {arXiv:2504.14891 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/kristensyme/Zotero/storage/WAHRELPZ/Gan et al. - 2025 - Retrieval Augmented Generation Evaluation in the E.pdf:application/pdf;Snapshot:/Users/kristensyme/Zotero/storage/EBE73PDF/2504.html:text/html},
}

@book{campbell_carving_2011,
	title = {Carving {Nature} at {Its} {Joints}: {Natural} {Kinds} in {Metaphysics} and {Science}},
	isbn = {978-0-262-29790-5},
	shorttitle = {Carving {Nature} at {Its} {Joints}},
	abstract = {Reflections on the metaphysics and epistemology of classification from a distinguished group of philosophers.Contemporary discussions of the success of science often invoke an ancient metaphor from Plato's Phaedrus: successful theories should "carve nature at its joints." But is nature really "jointed"? Are there natural kinds of things around which our theories cut? The essays in this volume offer reflections by a distinguished group of philosophers on a series of intertwined issues in the metaphysics and epistemology of classification.The contributors consider such topics as the relevance of natural kinds in inductive inference; the role of natural kinds in natural laws; the nature of fundamental properties; the naturalness of boundaries; the metaphysics and epistemology of biological kinds; and the relevance of biological kinds to certain questions in ethics. Carving Nature at Its Joints offers both breadth and thematic unity, providing a sampling of state-of-the-art work in contemporary analytic philosophy that will be of interest to a wide audience of scholars and students concerned with classification.},
	language = {en},
	publisher = {MIT Press},
	author = {Campbell, Joseph Keim and O'Rourke, Michael and Slater, Matthew H.},
	month = oct,
	year = {2011},
	note = {Google-Books-ID: 9wAsylWKUXwC},
	keywords = {Philosophy / Metaphysics},
}

@article{hacking_tradition_1991,
	title = {A tradition of natural kinds},
	volume = {61},
	url = {https://www.jstor.org/stable/4320173?casa_token=sjkrj-UcSTIAAAAA:Gpd95V26BEikyBRLz6xT9LSwXseGmTBkyd9euSewggx_v5CJbX5xhN5xer9kzmXjSOBfpjEu0Cs_xfuAoc4TbMaBTps-dulG__YwxID3dpo1ffkbVA},
	number = {1/2},
	urldate = {2025-05-22},
	journal = {Philosophical Studies: An International Journal for Philosophy in the Analytic Tradition},
	author = {Hacking, Ian},
	year = {1991},
	note = {Publisher: JSTOR},
	pages = {109--126},
}

@article{liu2021,
  title={P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks},
  author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2110.07602},
  year={2021}
}